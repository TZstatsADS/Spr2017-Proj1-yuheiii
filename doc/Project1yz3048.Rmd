---
title: "Project1-YuxiZhou-yz3048"
output:
  html_document: default
  html_notebook: default
---

#Step 0 - Install and load libraries
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

JAVA_HOME<- "C:/Software/JRE/JAVA/bin"
# load packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library(tidyr)

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

This notebook was prepared with the following environmental settings.

```{r}
print(R.version)
```

# Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.
```{r, message=FALSE, warning=FALSE}
### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches. 
inaug=f.speechlinks(main.page)
#head(inaug)
Sys.setlocale("LC_TIME","us")
as.Date(inaug[,1], format="%B %e, %Y")
```
```{r}
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.

#### Nomination speeches
main.page=read_html("http://www.presidency.ucsb.edu/nomination.php")
# Get link URLs
nomin <- f.speechlinks(main.page)
#head(nomin)
#
#### Farewell speeches
main.page=read_html("http://www.presidency.ucsb.edu/farewell_addresses.php")
# Get link URLs
farewell <- f.speechlinks(main.page)
#head(farewell)
```

Step 2: Using speech metadata posted on http://www.presidency.ucsb.edu/, we prepared CSV data sets for the speeches we will scrap.
```{r}
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
nomin.list=read.csv("../data/nominlist.csv", stringsAsFactors = FALSE)
farewell.list=read.csv("../data/farewelllist.csv", stringsAsFactors = FALSE)
```
We assemble all scrapped speeches into one list. Note here that we don¡¯t have the full text yet, only the links to full text transcripts.

Step 3: scrap the texts of speeches from the speech URLs.
```{r}
speech.list=rbind(inaug.list, nomin.list, farewell.list)
speech.list$type=c(rep("inaug", nrow(inaug.list)),
                   rep("nomin", nrow(nomin.list)),
                   rep("farewell", nrow(farewell.list)))
speech.url=rbind(inaug, nomin, farewell)
speech.list=cbind(speech.list, speech.url)
```

Save scrapped speeches into our local folder as individual speech files.
```{r}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/fulltext/", 
                     speech.list$type[i],
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```


Manually add several public transcripts from Trump.
```{r, message=FALSE, warning=FALSE}
speech1=paste(readLines("../data/fulltext/SpeechDonaldTrump-NA.txt", 
                  n=-1, skipNul=TRUE),
              collapse=" ")
speech2=paste(readLines("../data/fulltext/SpeechDonaldTrump-NA2.txt", 
                  n=-1, skipNul=TRUE),
              collapse=" ")
speech3=paste(readLines("../data/fulltext/PressDonaldTrump-NA.txt", 
                  n=-1, skipNul=TRUE),
              collapse=" ")

Trump.speeches=data.frame(
  President=rep("Donald J. Trump", 3),
  File=rep("DonaldJTrump", 3),
  Term=rep(0, 3),
  Party=rep("Republican", 3),
  Date=c("August 31, 2016", "September 7, 2016", "January 11, 2017"),
  Words=c(word_count(speech1), word_count(speech2), word_count(speech3)),
  Win=rep("yes", 3),
  type=rep("speeches", 3),
  links=rep(NA, 3),
  urls=rep(NA, 3),
  fulltext=c(speech1, speech2, speech3)
)

names(speech.list) <- names(Trump.speeches)
speech.list <- rbind(speech.list, Trump.speeches)
```
Step 4: Data processing
We use sentence as units of analysis for the project.
```{r, message=FALSE, warning=FALSE}
#assign an sequential id to each sentence in a speech (sent.id) and also calculated the number of words in each sentence as sentence length (word.count).

sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    word.count=word_count(sentences)

    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count))

```

Step 5  data analysis
```{r}
sel.comparison=c("DonaldJTrump","JohnMcCain", "GeorgeBush", "MittRomney", "GeorgeWBush",
                 "RonaldReagan","AlbertGore,Jr", "HillaryClinton","JohnFKerry", 
                 "WilliamJClinton","HarrySTruman", "BarackObama", "LyndonBJohnson",
                 "GeraldRFord", "JimmyCarter", "DwightDEisenhower","FranklinDRoosevelt",
                 "HerbertHoover","JohnFKennedy","RichardNixon","WoodrowWilson", 
                 "AbrahamLincoln", "TheodoreRoosevelt", "JamesGarfield", 
                 "JohnQuincyAdams", "UlyssesSGrant", "ThomasJefferson",
                 "GeorgeWashington", "WilliamHowardTaft", "AndrewJackson",
                 "WilliamHenryHarrison", "JohnAdams")
```


```{r}
par(mar=c(4, 11, 2, 2))
sentence.list$File=factor(sentence.list$File)
#sentence.list$FileOrdered=reorder(sentence.list$File, sentence.list$word.count, mean, order=T)
findlove <-c()
for(i in 1:nrow(sentence.list)){
  findlove[i] <-sum(grepl("love", sentence.list$sentences[i]))
}
sentence.list <-cbind(sentence.list, findlove)
sumfindlove <-c()
for(i in 1: length(levels(sentence.list$File))){
  sumfindlove[i] <-sum(sentence.list$findlove[sentence.list$File ==levels(sentence.list$File)[i]])
}
beeswarm(sumfindlove ~levels(sentence.list$File), 
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.8, cex.lab=0.8,
         spacing=5/nlevels(sentence.list$File),
         las=2, xlab="Number of word 'love' in speech", ylab="",
         main="Word 'love' statistics")

```

Similarly, we can get the statistical result of every word we want. As I suppose that "love" and "hate" are the two most powerful words that are used in the speech.
```{r}
findhate <-c()
for(i in 1:nrow(sentence.list)){
  findhate[i] <-sum(grepl("hate", sentence.list$sentences[i]))
}
sentence.list <-cbind(sentence.list, findhate)
sumfindhate <-c()
for(i in 1: length(levels(sentence.list$File))){
  sumfindhate[i] <-sum(sentence.list$findhate[sentence.list$File ==levels(sentence.list$File)[i]])
}
beeswarm(sumfindhate ~levels(sentence.list$File), 
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.8, cex.lab=0.8,
         spacing=5/nlevels(sentence.list$File),
         las=2, xlab="Number of word 'hate' in speech", ylab="",
         main="Word 'hate' statistics")
```

Also, my purpose is to do another research on political sensitive words mentioned in the speeches such as immigrant, war&peace, etc. But as time is limited, I will have this part after due. 

In the result of the reasearch, Donald Trump and George Bush provide the most times of "love" words in their speeches. As the"emotional brand" and "personality" being more and more crucial on people's choice of their tickets, the president choose to have a nicer image of their appearance. However, the words used with strong emotion cannot change the way people firstly thought of them.

As time is limited since so much stress on first weeks to catch up with every sujects, and limited ability, I just tried all I can to understand Prof's code and think of what else I can do. I'll catch up as hard as possible, sorry if the project can not meet the request of the class.